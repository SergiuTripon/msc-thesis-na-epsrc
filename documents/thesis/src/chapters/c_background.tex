\chapter{\textsc{Background}}
\label{chapter:background}

In this chapter, background information regarding EPSRC is provided, while several concepts that are both related to and employed in the work carried out in this project, are also introduced.

\section{EPSRC}

\textit{EPSRC} is one of seven research councils in the United Kingdom including \textit{Economic and Social Research (ESRC)} and \textit{Medical Research (MRC)} that form \textit{Research Councils UK (RCUK)}, a non-departmental government body. RCUK's purpose is to manage the relationship between seven separate research councils which fund research projects in various disciplines.

EPSRC provides funding for research grants and training in two main disciplines: engineering and the physical sciences. Yearly, it invests more than \pounds800 million in a variety of subjects ranging from mathematics to materials science, and from information technology to structural engineering \cite{epsrc_about_us}. It also boasts support worth \pounds2-3 billion for a portfolio of research and training \cite{epsrc_our_portfolio}.

Currently, EPSRC holds a grant portfolio consisting of 3,175 grants. Each grant represents a research project within which one or more researchers collaborate, and it consists of substantial information including topic classifications. In total, the 3,175 grants are classified using 225 current topics. Researchers submit funding proposals to EPSRC, which, once accepted, turn into research grants financially supported by EPSRC.

\clearpage

\section{Text-based methods}

There are several types of clustering and classification methods, and most of them are text-based. This section introduces a number of them including \textit{document clustering}, \textit{topic Modelling} and \textit{document classification}.

\subsection{Topic Modelling}

\textit{Topic modelling} is a popular form of \textit{text mining} used in \textit{machine learning} and \textit{natural language processing} to discover patterns in a text corpus. Naturally, a text body focusing on a particular topic such as computer games will contain certain words more or less frequently, \textit{"graphics"} and \textit{"animation"} more than \textit{"shoes"} and \textit{"dresses"}, for example. Other words such as "\textit{"is"} and \textit{"the"}, also known as stop words, will appear frequently regardless of the topic, and are usually removed from the corpus due to their low value.

\subsection{Document clustering}

\textit{Document clustering} is the task of dividing a document collection into a number of different clusters of documents based on similarity as a function of a document. It is a popular application in many areas such as information retrieval and topic extraction. There are clear differences between the classification and clustering of documents. The aim of document clustering algorithms is to divide a document collection into clusters of documents that hold a coherent structure. In contrast, classification is focused on discovering the type of a document by using its features.

Similarly to \textit{topic modelling}, document clustering also analyses the text body of a document. However, there is a difference in motivation, as topic modelling is used to discover trends within text which could then be modelled into a number of topical keywords, representing the text as a whole.

\subsection{Other forms of classification}

\textit{Classification} is the action of categorising a collection of entities into separate categories based on some criteria such as similarity. A \textit{classification} represents an ordered list of the categories used to group the entities. Moreover, a \textit{classification} system is a method of realising \textit{classification}. Furthermore, \textit{classification} plays a valuable part in various subjects including \textit{mathematics}, \textit{media}, \textit{science} and \textit{business}. This section introduces a number of different forms of \textit{classification} which share some common ground with the task that this project aims to accomplish.

\subsubsection{Library classification}

\textit{Library classification} is the task of organising library material by subject or topic. Items are stored according to the order of the topics in the \textit{classification}, which is represented by a \textit{notational} system. This means that related materials are grouped in the same category, usually following a hierarchical tree structure.

In a library environment, the person responsible for classifying library materials is known as a \textit{library cataloguer} or a \textit{catalogue librarian}. The classification of a library material consists of two stages. Firstly, the cataloguer needs to find out what the material is about. This is followed by the material being assigned a call number by the \textit{notational} system, which can be perceived as the address of a book. A library material can only be located in one physical space at a time, which means it can also only be assigned to one category at a time. In contrast, alphabetical indexing languages such as \textit{Thesauri} or \textit{Subject Headings} systems allow materials to be labelled with multiple terms.

\subsubsection{Document Classification}

\textit{Document classification} is a \textit{classification} problem in the fields of \textit{library}, \textit{information} and \textit{computer} science. It deals with the task of assigning a document to one or more categories \textit{manually} and \textit{intellectually} or \textit{algorithmically}.

\textit{Document classification} is used in \textit{library} classification where a \textit{catalogue librarian} \textit{manually} and \textit{intellectually} determines what a library material is about, which results in its classification. In computer science, \textit{document classification} is accomplished \textit{computationally} through the use of various \textit{document classification} algorithms.

\section{Connectivity-based methods}

In the previous section, a number of text-based clustering and classification methods were introduced. In contrast, this project does not aim to solve the problem defined using text-based methods, but connectivity-based methods such as network analysis, graph theory, the concept of modularity and community detection.

There are clear similarities and differences between the text and connectivity based methods. Firstly, this project holds contextual differences when compared to topic modelling and its applications. The former analyses a network of topics and aims to divide it into a number of coherent clusters while the latter is used to identify patterns as potential topics underlying a text corpus. Furthermore, \textit{document clustering} and \textit{community detection} have clear similarities in terms of the resulting rational clustering. In both methods, the members of a cluster hold a strong relationship. This is supported by weaker links and decreased similarity between members of different clusters. However, \textit{document clustering} is based on the analysis of documents through a number of different techniques such as \textit{tokenization}, \textit{stemming}, \textit{lemmatization}, removal of \textit{stop words} and \textit{punctuation}, and the computation of \textit{term frequencies}. On the other hand, \textit{community detection} is concerned with the structure of a network.

Subsequently, \textit{library classification} holds concrete similarities with the motivation behind this project. EPSRC can be perceived as a substitute for a library, while grants are the equivalent of library materials. Both library materials and grants are classified using topics. Grants can be classified using one or more topics, while library materials must be assigned to a single category. Both forms of \textit{classification} are performed manually and intellectually, one by a catalogue librarian and the other by researchers during the process of making a proposal for funding. However, the purpose of this project is not the classification of grants, but the grouping of research topics into different research areas. Moreover, the project aims to achieve this computationally, employing the human judgement underlying the data and a novel approach to the problem involving graph theory. In contrast, this is a step further than the library classification task where the process of classifying library materials is carried out manually by a human being.

Finally, \textit{computational document classification} has similarities with the application of community detection as both are based on algorithms and achieved computationally. However, differences exist in terms of what is classified. Document classification aims to determine a document's category, while this project seeks to discover the research area representing a group of topics.

This section introduces a number of different network science sub-fields which are used throughout the project such as \textit{network community structure}, \textit{community detection} and the \textit{concept of modularity}.

\subsection{Network Community and Community Structure}

A \textit{network community} is a group of nodes which are densely-connected between each other and sparsely connected to other nodes in a network. Moreover, a \textit{network community structure} is a group of network communities that is identified in a network.

Detecting the community structure in a network is one of several tasks in \textit{network science}. Over the years, the task has become increasingly popular which led to the birth of a large number of community detection algorithms including \textit{Spinglass}, \textit{Louvain}, \textit{Fast Greedy} and \textit{Infomap}. A community detection algorithm divides a network into a number of clusters, which may be overlapping. If the nodes within each identified cluster are densely connected, it means that the network holds a community structure. In the case of no overlapping clusters, it means that the network is naturally divided and nodes within each cluster are densely connected while nodes between clusters are sparsely connected. It is assumed that two nodes are more likely to be connected if they share the same cluster, and less likely if they do not.

\subsection{The concept of Modularity}

\textit{Modularity}, introduced by M.E.J. Newman \cite{newman2006modularity}, is one way to measure the strength of a community structure identified by a community detection algorithm. High modularity in networks means that nodes within each cluster are densely connected, while being sparsely connected to nodes in other clusters. Therefore, a network that holds a community structure is likely to also hold a high modularity. The motivation behind the concept comes from the analysis of social and biological networks which are known to hold a community structure. Identifying the community structure of such networks is invaluable in the journey of seeking a deeper understanding of a network's dynamics. For example, in a social network, information is more likely to travel faster within a community formed of densely connected nodes compared to a community composed of sparsely connected ones.

Initially, the modularity function solved the problem of dividing a network in two communities. The function was then modified so that it could also apply to the problem of network division into two or more communities. The concept of modularity is an extremely beneficial breakthrough in the network division problem.

However, it also has a resolution limit which leads to the inability of detecting small communities. During the process of dividing a network, a null model version of the network in question is created. A null model is an instance of a random graph which shares the same features as a specific real graph. The number of edges in a cluster within the real network is compared to the number of edges in a cluster within the null model. The null model assumes that each node can be linked to any other node in the network, which, if the network is large, is not necessarily true. 

Regardless, the concept of modularity remains important and relevant as community detection algorithms often incorporate it in order to identify and measure the strength of a network's community structure.