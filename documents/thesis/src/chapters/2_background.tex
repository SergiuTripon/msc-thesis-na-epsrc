\chapter{\textsc{Background}}
\label{chapterlabel2}

In the previous chapter, the project was introduced and the problem was defined. This chapter provides detailed background information about EPSRC and the problem that this project aims to solve using several concepts including graph theory, modularity and community structure and detection, which are also presented. Furthermore, other fields related to this study are also briefly introduced.

\section{EPSRC}

Engineering and Physical Sciences Research Council (EPSRC) is one of seven research councils in the United Kingdom including Economic and Social Research (ESRC) and Medical Research (MRC) that form Research Councils UK (RCUK), a non-departmental government body. RCUK's purpose is to manage the relationship between seven separate research councils which manage and fund research projects in various disciplines. It ensures effective collaborative work between the councils enhancing the overall impact of their research, training, and innovation.

EPSRC provides funding for research grants and training in two main disciplines: engineering and the physical sciences. Yearly, it invests more than \pounds800 million in a variety of subjects ranging from mathematics to materials science, and from information technology to structural engineering \cite{epsrc_about_us}. It also boasts support worth \pounds2-3 billion for a portfolio of research and training \cite{epsrc_our_portfolio}.

Currently, EPSRC holds a grant portfolio consisting of 3175 grants. Each grant represents a research project within which one or more researchers collaborate and it consists of substantial information including topic classifications. In total, the 3175 grants are classified using 225 current topics. Researchers submit funding proposals to EPSRC, which once accepted turn into research grants financially supported by EPSRC.

During the process of making a proposal, one of the tasks that researchers complete is the classification of the project using a number of different topics. They are allowed to classify the project using one or more topics. By doing this, researchers use their judgement and provide intelligence which can prove to be extremely useful, when analysing a network of topics constructed based on the topic classification of grants. It is essential to note that a community detection method solely focuses on a network and its structure and does not use any other information. However, the structure of that network represents the human judgement and intelligence provided by researchers at the very beginning, during the process of making a proposal. 

Assume a problem concerning the way topics within EPSRC are defined and categorised. It is not known how fine or coarse the topics should be defined and how to define them in either way. This problem may arise when there are too many similar topics or not enough topics available to cover the full topic classification spectrum of the grants. The relation between topics is also unknown. One approach to solving such a problem is using graph theory and the concept of modularity. Focusing on grants that are classified by two or more topics, a network of topics can be constructed with the links between topics representing one or more common grants. Visually, imagine that the topics are represented by circles and the links between topics by lines.

Initially, within the network of topics constructed, there is no difference between topics, they are all treated and size the same. The only difference is in their names and links to other topics. Attempting to determine how topics should be defined while at the same time, focusing on the full network will prove extremely difficult. Subsequently, a potential alternative could involve grouping similar topics manually, and subsequently analysing each group separately. This would lower the difficulty level compared to the previous attempt, but will significantly increase the time it would take to complete the task. However, if a technique employed computationally would be employed to divide the network into a number of parts, the task would be completed efficiently and with ease. Using the same computational technique, each part could be divided further into a number of sub-parts in order to obtain a more refined clustering. Additionally, this would allow the analysis of specific parts of the network which will help decision-making regarding the definition of topics tremendously. Furthermore, this solution could also be used to analyse other networks like a network of researchers where the task would involve identifying communities of researchers.

This project aims to divide networks constructed from EPSRC data into clusters of topics representing researcher areas and clusters of researchers using graph theory and community detection methods. Achieving this will bring clarity, efficiency and substantial value to the process of defining topics within EPSRC.

\iffalse
In the financial year running from 1 April 2014 to 31 March 2015, EPSRC considered 2,386 research grants, funded 914 and rejected 1472. This represents a funding rate by number of 38\% and a rejection rate by number of 62\%. The total value of the proposals considered amounted to \pounds1,823 billion while the total value of the funded proposals was \pounds713 million. The total value of the rejected proposals was \pounds1,110 billion.

Furthermore, proposals from 104 organisations were considered. Imperial College London had the most proposals considered (151) and funded (58) but also the most rejected proposals (93) \cite{epsrc_funding_rates_1415}. They were followed by University College London with 120 proposals considered, 52 funded and 68 rejected.
\fi

\section{Network Community Structure}

Detecting the community structure in a network is one of several tasks in network science. Usually, the community structure is detected through the use of a community detection algorithm. Over the years, the task has become increasingly popular which led to the birth of a large number of community detection algorithms.

A community detection algorithm divides a network into a number of clusters, which may be overlapping. If the nodes within each cluster are densely connected, it means that the network holds a community structure. In the case of no overlapping clusters, it means that network is naturally divided and nodes within each cluster are densely connected while nodes between clusters are sparsely connected. It is assumed that two nodes are more likely to be connected if they share the same cluster, and less likely if they do not.

\section{The concept of Modularity}

Modularity, introduced by M.E.J. Newman \cite{newman2006modularity}, is one way to measure the strength of a community structure identified by a community detection algorithm. High modularity in networks means that nodes within each cluster are densely connected, while being sparsely connected to nodes in other clusters. Therefore, a network that holds a community structure is likely to also hold a high modularity. The motivation behind the concept comes from the analysis of social and biological networks which are known to hold a community structure. Identifying the community structure of such networks is invaluable in the journey of seeking a deep understanding of a network's dynamics. For example, in a social network, information is more likely to travel faster within a community formed of densely connected nodes compared to sparsely connected ones.

Initially, the modularity function solved the problem of dividing a network in two communities. The function was then modified so that it could also apply to the problem of network division into two or more communities. The concept of modularity is an extremely beneficial breakthrough in the network division problem. However, it also has a resolution limit which leads to the inability of detecting small communities. During the process of dividing a network, a null model version of the network in question is created. A null model is an instance of a random graph which shares the same features as a specific real graph. The number of edges in a cluster within the real network is compared to the number of edges in a cluster within the null model. The null model assumes that each node can be linked to any other node in the network. However, if the network is large, this is not necessarily true. 

Regardless, the concept of modularity remains important and relevant as community detection algorithms often incorporate it in order to identify and measure the strength of a network's community structure.

\iffalse
\section{Network Science}

Network science is an academic field focusing on the analysis and study of complex networks including telecommunication, computer, biological and social networks. The field combines theories and methods with origins in mathematics, physics, computer science and sociology such as graph theory, statistical inference, data mining and information visualisation and social structure.

Moreover, the research field of network science is composed by a large number of specific studies that focus their purpose on a certain aspect of network science including network theory and analysis.
\fi

\iffalse
\subsection{Network Theory}

Network Theory is part of graph theory and studies networks as graphs comprised of nodes connected by either directed (asymmetric) or undirected (symmetric) edges. Network Theory is employed in various other fields such as computer science, electrical engineering, biology and economics.
\fi

\iffalse
\subsection{Network Analysis}

Network Analysis is a sub-field of network science and focuses on the analysis of networks from a number of different perspectives. Different types of network analysis exist including social, dynamic, biological, link, pandemic and web link analysis. Two increasingly popular network analysis topics are social and biological network analysis. Social network analysis (SNA) focuses on observing the relationship between social actors within networks which are usually represented by humans but also organisations, website and publications. Closely related to SNA, biological network analysis is used significantly in the analysis of molecular networks.

A network primarily consists of two entities: nodes or vertices and edges or links. Nodes or vertices, usually represented by circles, represent the main distinct actors, stations within a transport network, for example. Nodes can vary in size depending on how the node size property is assigned. Typically, the degree of a node shapes its size but other attributes specific to the research project in question can be used.

Subsequently, edges represent the connections between the actors of a network. Edges can be directed or undirected. A directed edge allows the source node to reach the target node, but the target node cannot reach the source node. In contrast, nodes can "travel" to other nodes without a direction limitation when connected by undirected edges. Similarly to nodes, edges can also vary in width. An edge's width is usually controlled by the number of common entities that two nodes have in common, and is indicative of how strong the relationship between two nodes is.

Furthermore, every network holds a number of properties which can be calculated in order to explore and observe its characteristics. Most of these calculations exist in the form of algorithms which are applied through the use of tools built for the purpose of analysis and visualisation.
\fi

\section{Network Community Detection Algorithms}

Community detection is a popular subject in the study of networks and this is accurately represented in the significant number of community detection algorithms that have been and are still developed. This project initially considered and compared eight community detection algorithms including \textit{Infomap} and \textit{Walktrap}. Following an extensive comparative analysis, the number of algorithms was reduced to a final three: \textit{Louvain}, \textit{Spinglass} and \textit{Fast Greedy}. The results identified the \textit{Louvain} method as the optimal community detection algorithm. This section provides further information regarding the final three community detection algorithms.

\iffalse
\subsection{Clustering Coefficient}

The clustering coefficient algorithm is used to identify clustering in a network. It is a numerical value between 0 and 1, with 0 indicating no clustering and 1 indicating high clustering. Clustering within a network occurs when nodes group together through a high proportion of 10 edges between them. The clustering coefficient is commonly identified by searching for triangles. In a highly clustered network, when two edges have a common node, there is probably a third edge and the three edges form a triangle. This study uses the algorithm to determine if the network is a small-world network. Alongside a high clustering coefficient, a small average path length indicates that a network is in fact a small-world network.
\fi

\subsection{Louvain introduced by Blondel et al.}

\textbf{Louvain} is a modularity optimisation algorithm introduced by \textit{Blondel et al.} \cite{blondel2008fast}. It proposes a two-phase hierarchical agglomerative approach which is an improvement of Fast Greedy by Newman et al. \cite{newman2004fast} The first phase of the algorithm involves the application of a greedy optimisation in order to detect communities. In the second phase, a new network is constructed using the communities found during the first phase as nodes. Edges between communities are represented as self-loops, while edges within communities are summed and represented as edges between the new nodes. This process is repeated until a single community remains \cite{orman2011accuracy}.

\subsection{Spinglass introduced by Reichardt and Bornholdt}

\textbf{Spinglass} is another modularity optimisation proposed by \textit{Reichardt and Bornholdt} \cite{reichardt2006statistical} which is based on a combination between a popular statistical mechanic model called Potts spin glass, and community structure. The algorithm applies the technique of simulated annealing on Potts in order to achieve an optimal modularity \cite{orman2011accuracy}.

\subsection{Fast Greedy introduced by Newman et al.}

\textbf{FastGreedy}, an algorithm developed by \textit{Newman et al.} \cite{newman2004fast} is based on a greedy optimization method applied to a hierarchical agglomerative approach. Initially, each node represents its own community. The communities are merged by the algorithm step by step until only one remains, containing all nodes. The greedy approach is applied at each step, by considering the largest increase or smallest decrease in modularity as the criteria for merging. Due to the algorithm's hierarchical nature, it produces a hierarchy of community structures. The comparison of modularity values determines the best community structure \cite{orman2011accuracy}.

\section{Other forms of classification}

Classification is the action of categorising a collection of entities into separate categories based on some criteria such as similarity. A classification represents an ordered list of the categories used to group the entities. A classification system is a method of realizing classification. Classification plays a valuable part in various subjects including mathematics, media, science and business. This section introduces a number of different forms of classification which share some common ground with the task that this project aims to accomplish.

\subsection{Library classification}

Library classification is the task of organising library material by subject or topic. Items are stored according to the order of the topics in the classification, which is represented by a notational system. This means that related materials are grouped in the same category, usually following a hierarchical tree structure.

In a library environment, the person responsible for classifying library materials is known as a library cataloguer or a catalogue librarian. The classification of a library material consists of two stages. Firstly, the cataloguer needs to find out what the material is about. This is followed by the material being assigned a call number by the notational system, which can be perceived as a book's address. A library material can only be located in one physical space at a time, which means it can also only be assigned to one category at a time. In contrast, alphabetical indexing languages such as \textit{Thesauri} or \textit{Subject Headings} systems allow materials to be labelled with multiple terms.

This form of classification holds concrete similarities with the motivation behind this project. EPSRC can be perceived as a substitute for a library, while grants are the equivalent of library materials. Both library materials and grants are classified using topics. Grants can be classified using one or more topics, while library materials must be assigned to a single category. Both forms of classification are performed manually, one by a catalogue librarian and the other by researchers during the process of making a proposal for funding. However, the purpose of this project is not the classification of grants, but the grouping of research topics into different research areas. Moreover, the project aims to achieve this computationally employing the human judgement underlying the data and a novel approach to the problem involving graph theory. In contrast, this is a step further than the library classification task where the process of classifying library materials is manually carried out by a human being.

\subsection{Document Classification}

Document classification is a classification problem in the fields of library science, information science and computer science. It deals with the task of assigning a document to one or more categories manually and intellectually or algorithmically.

Document classification is used in library classification where a catalogue librarian manually and intellectually determines what a library material is about which results in its classification. As previously mentioned, this can be considered the equivalent of researchers using rationale and manually classifying funding proposals using a number of different topics.

In computer science, document classification is accomplished computationally through the use of various document classification algorithms. This application has similarities to the application of community detection as both are based on algorithms and achieved computationally. However, differences exists in terms of what is classified. Document classification aims to determine a document's category, while this project seeks to discover the research area representing a group of topics.

\section{Document clustering}

Document clustering is the task of dividing a document collection into a number of different clusters of documents based on similarity as a function of a document. It is a popular application in many areas such as information retrieval and topic extraction. There are clear differences between the classification and clustering of documents. The aim of document clustering algorithms is to divide a document collection into clusters of documents that hold a coherent structure. In contrast, classification is focused on discovering the type of a document by using its features.

Document clustering and community detection have clear similarities in terms of the resulting rational clustering. In both methods, the members of a clustering hold a strong relation. This is supported by weaker links and decreased similarity between members of different clusters. However, document clustering is based on the analysis of documents through a number of different techniques such as tokenization, stemming and lemmatization, removal of stop words and punctuation and the computation of term frequencies. On the other hand, a community detection algorithm is applied to a network and results in a clustering also known as a community structure, which is based on the structure and dynamics of the network.

\section{Topic Modelling}

Topic Modelling is a popular form of text mining used in machine learning and natural language processing to discover patterns in a text corpus. Naturally, a text body focusing on a particular topic such as computer games will contain certain words more or less frequently, graphics and animation more than shoes and dresses, for example. Other words such as "is" and "the", also known as stop words, will appear frequently regardless of the topic, and are usually removed from the corpus due to their low value.

Similarly to document clustering, topic modelling also analyses the text body of a document. However, there is a difference in motivation as topic modelling is used to discover trends within text which could then be modelled into a number of topical keywords, representing the text as a whole. 

Additionally, this project holds contextual differences when compared to topic modelling and its applications. The former analyses a network of topics and aims to divide it into a number of coherent clusters while the latter is used to identify patterns as potential topics underlying a text corpus.