\chapter{Source Code}
\label{appendix:code}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstdefinestyle{Python}{frame=tb,
  language=Python,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}

\lstdefinestyle{Bash}{frame=tb,
  language=bash,
  basicstyle=\small\ttfamily,
  showstringspaces=false,
  commentstyle=\color{red},
  keywordstyle=\color{blue}
}

During this project, source code was written in Python and Bash and stored within \textbf{12 files (6,123 lines)}, as follows:

\begin{enumerate}[itemsep=0cm, label*=\arabic*.]
    \item \textbf{Data collection (2,541 lines)}
\begin{enumerate}[itemsep=0cm, label*=\arabic*.]
    \item \textbf{download.sh (153 lines)}, written to download grant and research records from the \textit{EPSRC GoW service}
    \item \textbf{extract.py (964 lines)}, written to extract information from the downloaded data
    \item \textbf{link.py (844 lines)}, written to establish the links between the topics and researchers
    \item \textbf{make.py (414 lines)}, written to create the \textit{Topic} and \textit{Researcher networks}
    \item \textbf{run.py (35 lines)}, written to run the \textit{extract.py}, \textit{link.py}, \textit{make.py} functions from one central file
    \item \textbf{check.sh (131 lines)}, written to ensure correct file creation
\end{enumerate}
    \item \textbf{Network analysis (3,582 lines)}
\begin{enumerate}[itemsep=0cm, label*=\arabic*.]
    \item \textbf{network.py (711 lines)}, written to analyse the network
    \item \textbf{communities.py (598 lines)}, written to analyse the communities within the network
    \item \textbf{sub\_communities.py (423 lines)}, written to analyse the sub-communities in each community within the network
    \item \textbf{analysis.py (1,176 lines)}, written to bind the \textit{network.py}, \textit{communities.py}, \textit{sub\_communities.py} functions together in one central file
    \item \textbf{evaluation.py (173 lines)} written to evaluate the community structure identified
    \item \textbf{clean.sh (501 lines)}, written to delete multiple files from specific folders
\end{enumerate}
\end{enumerate}

\section{Source code location}

The source code is stored in a GitHub repository located at the following web address:

\url{https://github.com/SergiuTripon/msc-thesis-na-epsrc}

\section{Running the source code}

\textbf{Note: In order to run the source code, an virtual environment installation is required. The code is written in Python 3.5. The packages used in the project are listed in the \textit{requirements.txt} file and can be install using \textit{pip}}.

\vspace{1em}

\noindent Running the network analysis is achieved by running the \textit{analysis.py} file with the desired parameters (\textbf{-n} requires network (topic or researcher), \textbf{-i} requires interpretation (grants, researchers or topics), \textbf{-d} requires data set (1990-2000, 2000-2010, 2010-2016)), following the steps below:

\begin{lstlisting}[style=Bash]
# activate virtual environment
$ source venv/bin/activate

# navigate to analysis source folder
$ cd msc-thesis-na-epsrc/analysis/src/

# analyse topic (grants as edges, 2010-2016)
$ python analysis.py -n topic -i grants -d 2010-2016
# analyse topic (grants as edges, 2000-2010)
$ python analysis.py -n topic -i grants -d 2000-2010
# analyse topic (grants as edges, 1990-2000)
$ python analysis.py -n topic -i grants -d 1990-2000

# analyse topic (researchers as edges, 2010-2016)
$ python analysis.py -n topic -i researchers -d 2010-2016
# analyse topic (researchers as edges, 2000-2010)
$ python analysis.py -n topic -i researchers -d 2000-2010
# analyse topic (researchers as edges, 1990-2000)
$ python analysis.py -n topic -i researchers -d 1990-2000

# analyse researcher (grants as edges, 2010-2016)
$ python analysis.py -n topic -i grants -d 2010-2016
# analyse researcher (researchers as edges, 2000-2010)
$ python analysis.py -n topic -i grants -d 2000-2010
# analyse researcher (researchers as edges, 1990-2000)
$ python analysis.py -n topic -i grants -d 1990-2000

# analyse researcher (topics as edges, 2010-2016)
$ python analysis.py -n topic -i topics -d 2010-2016
# analyse researcher (topics as edges, 2000-2010)
$ python analysis.py -n topic -i topics -d 2000-2010
# analyse researcher (topics as edges, 1990-2000)
$ python analysis.py -n topic -i topics -d 1990-2000
\end{lstlisting}

\section{GitHub Wiki}

A GitHub Wiki was created to accompany this project and is located at the following web address: 

\url{https://github.com/SergiuTripon/msc-thesis-na-epsrc/wiki}

\noindent \textbf{Note: The information on the GitHub Wiki may be out-of-date}.

\section{Code snippets}

This section presents two code snippets which are considered particularly important. The first one achieves the contrast between grants as edges and grant records, while the second normalises the values of the node and edge attributes.

\clearpage

\subsection{Contrast between grants as edges and grant records}

This snippet of code was written in order to convert edges within the network in grants. Moreover, it calculates the number and value of grants within communities, between communities and within the entire network. Achieving this was extremely beneficial as it enabled the analysis of research trend and funding on the current (2010 to 2016) and historical (1990 to 2000, 2000 to 2010) data sets.

\begin{lstlisting}[caption={Code snippet showing function written to turn edges into grants and calculate the number and value of grants within network and communities and between communities.}\label{listing:turn_edges_into_grants}, style=Python]
# turns edges into grants
def turn_edges_into_grants(community, edge_type, method, count1, path):

    # variable to split path
    path_split = path.split('/', 3)

    # variable to hold temporary path
    path_temp = ''

    # if split path equals to current
    if path_split[1] == 'current':
        # set temporary path
        path_temp = '{}'.format(path_split[1])
    # if split path equals to past
    elif path_split[1] == 'past':
        # set temporary path
        path_temp = '{}/{}'.format(path_split[1], path_split[2])

    # variable to hold input file
    input_file = open(r'../../network-maker/output/grants/'
    '{}/info/grant_{}.pkl'.format(path_temp, path_split[0]), 'rb')
    # load data structure from file
    grant_entities = load(input_file)
    # close input file
    input_file.close()

    # variable to hold entity links
    entity_links = [[community.vs['label'][edge.source], community.vs['label'][edge.target]]
                    for edge in community.es()]

    # variable to hold grants
    grants = OrderedDict()

    # if split path equals to topics
    if path_split[0] == 'topics':

        # set grants
        grants = OrderedDict((ref, attr[1]) for entity_link in entity_links for ref, attr in grant_entities.items()
                             if entity_link[0] and entity_link[1] in attr[0])

    # if split path equals to researchers
    elif path_split[0] == 'researchers':

        # set grants
        grants = OrderedDict((ref, attr[1]) for entity_link in entity_links for ref, attr in grant_entities.items()
                             if entity_link[0] and entity_link[1] in [researcher[0] for researcher in attr[0]])

    # variable to hold number
    number = len([ref for ref in grants.keys()])

    # set locale to Great Britain
    setlocale(LC_ALL, 'en_GB.utf8')

    # variable to hold value
    value = sum([attr for attr in grants.values()])

    # variable to hold output file
    output_file = open('../../data/networks/{}/communities/txt/{}/{}/'
                       'grants.txt'.format(path, edge_type, method), mode='a')

    # if count1 is equal to 1
    if count1 == 1:

        # write header to file
        output_file.write('> Number and value of grants in each community\n\n')

        # write grant number and value to file
        output_file.write('- Community {}:    {:>4d} {}\n'.format(count1, number, currency(value, grouping=True)))

    # if count1 is not equal to 1
    else:

        # write grant number and value to file
        output_file.write('- Community {}:    {:>4d} {}\n'.format(count1, number, currency(value, grouping=True)))

    # return number and value
    return grants, number, value
\end{lstlisting}

\subsection{Normalisation of node and edge attribute values}

This code snippet was written in order to normalise the values of the node and edge attributes. The value of grants attribute, in particular, consisted of very large values, up to 7 digits. By normalising the values of the node and edge attributes, working with such large values was avoided and, therefore, the development, analysis and visualisation process was improved.

\begin{lstlisting}[caption={Code snippet showing function written to normalise the values of the node and edge attributes.}\label{listing:norm_vals}, style=Python]
# normalises values
def norm_vals(vals, new_min, new_max):

    # variable to hold old minimum and maximum
    old_min, old_max = min(vals), max(vals)
    # variable to hold old and new range
    old_range, new_range = old_max - old_min, new_max - new_min

    int_vals = [int(val) for val in vals]

    # variable to hold new values
    new_vals = [round((((val - old_min) * new_range / old_range) + new_min), 0) for val in int_vals]

    # return new values
    return new_vals
\end{lstlisting}

\chapter{Supplementary material}
\label{appendix:supplementary_material}

Additionally to the appendix in the thesis report, a 1095-page document of supplementary material was also produced and can be accessed either in the submission or at the following web address:

\url{https://github.com/SergiuTripon/msc-thesis-na-epsrc/blob/master/documents/supplementary-material/15110029_sergiu_tripon_supplementary_material.pdf}